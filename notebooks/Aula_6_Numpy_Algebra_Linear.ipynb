{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/master?filepath=notebooks%2FAula_6_Numpy_Algebra_Linear.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendo assistirem a série *Essence of Linear Algebra* do canal do YouTube [3blue1brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)\n",
    "\n",
    "[![](https://i.ytimg.com/vi/kjBOesZCoqc/maxresdefault.jpg)](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "Given the equivalence between matrices and linear transformations, we can reinterpret the statement *linear algebra is about vectors and matrices* by saying *linear algebra is about vectors and linear transformations.* \n",
    "\n",
    "If high school math is about *numbers and functions*, then linear algebra is about *vectors and vector functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations\n",
    "\n",
    "Matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "\n",
    "Vector $x \\in \\mathbb{R}^{n}$\n",
    "\n",
    "Multiply the Matrix $\\mathbf{A}$ can be thought of as computing a *linear transformation* $T_A$ that takes $n$-vectors as inputs and produces $m$-vectors as outputs:\n",
    "$$T_{A}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$$\n",
    "\n",
    "### Notation:\n",
    "Instead of writing $\\vec{y}=T_{A}(\\vec{x})$ to denote the linear transformation $T_A$ applied to $\\vec{x}$, we can write $\\vec{y} = \\mathbf{A}\\vec{x}$. Since the matrix $\\mathbf{A}$ has $m$ rows, the result of the matrix-vector product is an $m$-vector. \n",
    "\n",
    "### Example\n",
    "\n",
    "$$\\mathbf{A} = \\left[ \\begin{array}{ll}{10} & {5} & {-5} \\\\ {20} & {10} & {-10} \\end{array}\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\vec{x} = \\left[ \\begin{array}{12}{1} \\\\ {2} \\\\ {3} \\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x3 Matrix\n",
    "A = np.matrix([[10,5,-5],\n",
    "               [20,10,-10]])\n",
    "\n",
    "# 3-rows vector\n",
    "x = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5, 10]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(A,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec{y} = \\left[ \\begin{array}{13}{5} & {10} \\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse\n",
    "When a matrix $\\mathbf{A}$ is square and invertible, there exists an inverse matrix $\\mathbf{A}^{−1}$ which undoes the effect of $\\mathbf{A}$ to restore the original input vector:\n",
    "$$A^{-1}(A(\\vec{x}))=A^{-1} A \\vec{x}=\\vec{x}$$\n",
    "\n",
    "Using the matrix inverse $\\mathbf{A}^{−1}$ to undo the effects of the matrix $\\mathbf{A}$ is analogous to using the inverse function $f^−1$ to undo the effects of the function $f$.\n",
    "\n",
    "### Properties of Matrix Inverse Operation\n",
    "* $(\\mathbf{A} + \\mathbf{B})^{-1} = \\mathbf{A}^{-1} + \\mathbf{B} ^{-1}$\n",
    "* $(\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}$\n",
    "* $(\\mathbf{ABC})^{-1} = \\mathbf{C}^{-1} \\mathbf{B}^{-1} \\mathbf{A}^{-1}$\n",
    "* $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$\n",
    "\n",
    "### Example 1\n",
    "Consider the linear transformation that multiplies the first components of input vectors by 3 and multiplies the second components by 5, as described by the matrix.\n",
    "$$\\mathbf{A}=\\left[\\begin{array}{ll}{3} & {0} \\\\ {0} & {5}\\end{array}\\right], \\quad \\mathbf{A}(\\vec{x})=\\left[\\begin{array}{ll}{3} & {0} \\\\ {0} & {5}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}} \\\\ {x_{2}}\\end{array}\\right]=\\left[\\begin{array}{l}{3 x_{1}} \\\\ {5 x_{2}}\\end{array}\\right].$$\n",
    "\n",
    "The inverse is:\n",
    "$$\\mathbf{A}^{-1}=\\left[\\begin{array}{cc}{\\frac{1}{3}} & {0} \\\\ {0} & {\\frac{1}{5}}\\end{array}\\right], \\quad \\mathbf{A}^{-1}(\\mathbf{A}(\\vec{x}))=\\left[\\begin{array}{cc}{\\frac{1}{3}} & {0} \\\\ {0} & {\\frac{1}{5}}\\end{array}\\right]\\left[\\begin{array}{c}{3 x_{1}} \\\\ {5 x_{2}}\\end{array}\\right]=\\left[\\begin{array}{c}{x_{1}} \\\\ {x_{2}}\\end{array}\\right]=\\vec{x}.$$\n",
    "\n",
    "The inverse martrix multiplies the first component by $\\frac{1}{3}$ and the second component by $\\frac{1}{5}$, which effectively undoes what $\\mathbf{A}$ did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0],\n",
       "       [ 0, 15]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0],\n",
    "              [0,5]])\n",
    "x = np.array([2,\n",
    "              3])\n",
    "A*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(A)*(A*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence of an inverse\n",
    "\n",
    "Not all matrices are invertible. Given a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, we can check whether it is invertible or not by computing its determinant:\n",
    "\n",
    "$$A^{-1} \\text { exists } \\quad \\text { if and only if } \\quad \\operatorname{det}(A) \\neq 0$$\n",
    "\n",
    "Calculating the determinant of a matrix serves as an *invertibility test*.\n",
    "\n",
    "The exact value of the determinant is not important; it could be big or small, positive or negative; so long as the determinant is nonzero, the matrix passes the *invertibility test*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  0.13333333, -0.26666667,  0.13333333],\n",
       "       [ 0.83333333,  0.33333333, -0.96666667,  0.13333333],\n",
       "       [ 0.83333333,  0.13333333, -0.76666667,  0.13333333],\n",
       "       [ 0.83333333,  0.13333333, -0.96666667,  0.33333333]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3, -2,  4, -2],\n",
    "                [5,  3, -3, -2],\n",
    "                [5, -2,  2, -2],\n",
    "                [5, -2, -3,  3]])\n",
    "\n",
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Matrix\n",
    "\n",
    "$\\mathbf{A}^{-1}\\mathbf{A}=I_n$ and $\\mathbf{AB} = \\mathbf{AB} = I_n$\n",
    "\n",
    "### Example\n",
    "$$\\mathbf{A}^{-1} \\mathbf{A}=\\left[\\begin{array}{ll}{1} & {0} \\\\ {0} & {1}\\end{array}\\right]=I_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "The matrix product $\\mathbf{AB}$ of matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times \\ell}$ and $\\mathbf{B} \\in \\mathbb{R}^{\\ell \\times n}$ consists of computing the dot product between each row of $\\mathbf{A}$ and each column of $\\mathbf{B}$:\n",
    "\n",
    "$$\\mathbf{C}=\\mathbf{A} \\mathbf{B} \\quad \\Leftrightarrow \\quad c_{i j}=\\sum_{k=1}^{\\ell} a_{i k} b_{k j}, \\forall i \\in[1, \\ldots, m], j \\in[1, \\ldots, n].$$\n",
    "\n",
    "$$\\left[\\begin{array}{ll}{a_{11}} & {a_{12}} \\\\ {a_{21}} & {a_{22}} \\\\ {a_{31}} & {a_{32}}\\end{array}\\right]\\left[\\begin{array}{ll}{b_{11}} & {b_{12}} \\\\ {b_{21}} & {b_{22}}\\end{array}\\right]=\\left[\\begin{array}{ll}{a_{11} b_{11}+a_{12} b_{21}} & {a_{11} b_{12}+a_{12} b_{22}} \\\\ {a_{21} b_{11}+a_{22} b_{21}} & {a_{21} b_{12}+a_{22} b_{22}} \\\\ {a_{31} b_{11}+a_{32} b_{21}} & {a_{31} b_{12}+a_{32} b_{22}}\\end{array}\\right] \\in \\mathbb{R}^{3 \\times 2}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  0],\n",
       "       [ 0, 25],\n",
       "       [ 3, 25]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0],\n",
    "              [0,5],\n",
    "              [1,5]])\n",
    "B = np.array([[3,0],\n",
    "              [0,5]])\n",
    "\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "\n",
    "The transpose matrix $\\mathbf{A}^T$ is defined by the formula $a{^T}_{ij} = a_{ji}$. In other words, we obtain the transpose by “flipping” the matrix through its diagonal:\n",
    "\n",
    "$$^{T}: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "$$\\left[\\begin{array}{lll}{\\alpha_{1}} & {\\alpha_{2}} & {\\alpha_{3}} \\\\ {\\beta_{1}} & {\\beta_{2}} & {\\beta_{3}}\\end{array}\\right]^{\\top}=\\left[\\begin{array}{ll}{\\alpha_{1}} & {\\beta_{1}} \\\\ {\\alpha_{2}} & {\\beta_{2}} \\\\ {\\alpha_{3}} & {\\beta_{3}}\\end{array}\\right]$$\n",
    "\n",
    "### Properties of Matrix Transpose Operation\n",
    "\n",
    "* $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B} ^ T$\n",
    "* $(\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T$\n",
    "* $(\\mathbf{ABC})^T = \\mathbf{C}^T \\mathbf{B}^T \\mathbf{A}^T$\n",
    "* $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^1)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0],\n",
       "       [0, 5, 5],\n",
       "       [5, 3, 3]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace\n",
    "The trace of an $n \\times n$ matrix is the sum of the $n$ values on its diagonal:\n",
    "\n",
    "$$\\operatorname{Tr}: \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}, \\quad \\operatorname{Tr}[\\mathbf{A}] \\equiv \\sum_{i=1}^{n} a_{i i}$$\n",
    "\n",
    "### Properties of the Trace Operation:\n",
    "* $\\operatorname{Tr}[\\alpha \\mathbf{A}+\\beta \\mathbf{B}]=\\alpha \\operatorname{Tr}[\\mathbf{A}]+\\beta \\operatorname{Tr}[\\mathbf{B}] \\quad \\text { (linear property) }$\n",
    "* $\\operatorname{Tr}[\\mathbf{AB}] = \\operatorname{Tr}[\\mathbf{BA}]$\n",
    "* $\\operatorname{Tr}[\\mathbf{ABC}]=\\operatorname{Tr}[\\mathbf{CAB}]=\\operatorname{Tr}[\\mathbf{BCA}] \\quad \\text { (cyclic property) }$\n",
    "* $\\operatorname{Tr}[\\mathbf{A}^T] = \\operatorname{Tr}[\\mathbf{A}]$\n",
    "* $\\operatorname{Tr}[\\mathbf{A}]=\\sum_{i=1}^{n} \\lambda_{i}$, where $\\{\\lambda_i\\}$ are the eigenvalues of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "np.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant\n",
    "The determinant of a matrix is a calculation that involves all the coefficients of the matrix, and whose output is a single number:\n",
    "\n",
    "$$\\operatorname{det}: \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}.$$\n",
    "\n",
    "The determinant describes the relative geometry of the vectors that make up the rows of the matrix. More specifically, the determinant of a matrix $\\mathbf{A}$ tells you the *volume* of a box with sides given by rows of $\\mathbf{A}$.\n",
    "\n",
    "The determinant of a $2 \\times 2$ matrix is:\n",
    "\n",
    "$$\\operatorname{det}(\\mathbf{A})=\\operatorname{det}\\left(\\left[\\begin{array}{ll}{a} & {b} \\\\ {c} & {d}\\end{array}\\right]\\right)=\\left|\\begin{array}{ll}{a} & {b} \\\\ {c} & {d}\\end{array}\\right|=a d-b c$$\n",
    "\n",
    "The quantity $ad − bc$ corresponds to the area of the parallelogram formed by the vectors $(a, b)$ and $(c, d)$. Observe that if the rows of $\\mathbf{A}$ point in the same direction, $(a, b) = \\alpha(c, d)$ for some $\\alpha \\in \\mathbb{R}$, then the area of the parallelogram will be zero. If the determinant of a matrix is nonzero then the rows the matrix are **linearly independent**.\n",
    "\n",
    "### Properties of Determinants\n",
    "* $\\operatorname{det}[\\mathbf{AB}] = \\operatorname{det}[\\mathbf{A}] \\operatorname{det}[\\mathbf{B}]$\n",
    "* $\\operatorname{det}[\\mathbf{A}] = \\prod_{i=1}^{n} \\lambda_i$, where $\\{\\lambda_i\\}$ are the eigenvalues of $\\mathbf{A}$ \n",
    "* $\\operatorname{det}[\\mathbf{A}^T] = \\operatorname{det}[\\mathbf{A}]$\n",
    "* $\\operatorname{det}[\\mathbf{A}^{-1}] = \\frac{1}{\\operatorname{det}[\\mathbf{A}]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABoAAAAOCAYAAAAxDQxDAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABFklEQVQ4EZVUiQ3CMAykFQMgukHZgGcCGAFWYAdG6CyMABtAuwFsAHSDclf1pBClTrAUkpx9Z8clybqum8iyLKuG9QvzAqOC/yl/bDb5TDQkqyG0d/Yz7B8YpTBrRpzJV5IjRX0hYDzhxcf9PWKi/BxBtANG069+f27Y7tASns6yKF+JdlB5B5TaAaPfsig/T6iWCeZjWVL5PJFEVL2rqVNarUviq3WueGhdhMA/sIKJVHWIp2p5r8YsiZ/jr6qWhdojbPTSpvLVuivKLQMl60T0WxblK9EZKuuA0gpY41QdCOmhOF+3HOF8bvwn6ANs6cSwlXwca2GagZn8KQJkrP6Ee6EWbrDfQqhRANYt/Pxed2HObPK/ZyPlwO6+VXYAAAAASUVORK5CYII=\n",
      "text/latex": [
       "$\\displaystyle 0.0$"
      ],
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross product as a determinant\n",
    "We can compute the cross product of the vectors $\\vec{v} = (v_1, v_2, v_3)$ and $\\vec{w} = (w_1, w_2, w_3)$ by computing the determinant of a special matrix. We place the symbols $\\hat{i}, \\hat{j}, \\hat{k}$ in the first row of the matrix, then write the coefficients of $\\vec{v}$ and $\\vec{w}$ in the second and third rows. After expanding the determinant along the first row, we obtain the cross product:\n",
    "\n",
    "$$\\begin{aligned} \\vec{v} \\times \\vec{w} &=\\left|\\begin{array}{ccc}{\\hat{i}} & {\\hat{j}} & {\\hat{k}} \\\\ {v_{1}} & {v_{2}} & {v_{3}} \\\\ {w_{1}} & {w_{2}} & {w_{3}}\\end{array}\\right| \\\\ &=\\hat{i}\\left|\\begin{array}{cc}{v_{2}} & {v_{3}} \\\\ {w_{2}} & {w_{3}}\\end{array}\\right|-\\hat{j}\\left|\\begin{array}{cc}{v_{1}} & {v_{3}} \\\\ {w_{1}} & {w_{3}}\\end{array}\\right|+\\hat{k}\\left|\\begin{array}{cc}{v_{1}} & {v_{2}} \\\\ {w_{1}} & {w_{2}}\\end{array}\\right| \\\\ &=\\left(v_{2} w_{3}-v_{3} w_{2}\\right) \\hat{i}-\\left(v_{1} w_{3}-v_{3} w_{1}\\right) \\hat{j}+\\left(v_{1} w_{2}-v_{2} w_{1}\\right) \\hat{k} \\\\ &=\\left(v_{2} w_{3}-v_{3} w_{2}, v_{3} w_{1}-v_{1} w_{3}, v_{1} w_{2}-v_{2} w_{1}\\right) \\end{aligned}$$\n",
    "\n",
    "Observe that the anti-linear property of the vector cross product $\\vec{v} \\times \\vec{w} = -\\vec{w} \\times \\vec{v}$ corresponds to the swapping-rows-changes-the-sign property of determinants.\n",
    "\n",
    "Using the correspondence between the cross-product and the determinant, we can write the determinant of a $3 \\times 3$ matrix in terms of the dot product and cross product:\n",
    "\n",
    "$$\\left|\\begin{array}{ccc}{u_{1}} & {u_{2}} & {u_{3}} \\\\ {v_{1}} & {v_{2}} & {v_{3}} \\\\ {w_{1}} & {w_{2}} & {w_{3}}\\end{array}\\right|=\\vec{u} \\cdot(\\vec{v} \\times \\vec{w})$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\\operatorname{det}[3 \\times 3] = \\vec{u} \\cdot(\\vec{v} \\times \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues\n",
    "The determinant operation is used to define the *characteristic polynomial* of a matrix. The *characteristic polynomial* of $\\mathbf{A}$ is:\n",
    "\n",
    "$$\\begin{aligned} p_{\\mathbf{A}}(\\lambda) & \\equiv \\operatorname{det}(\\mathbf{A}-\\lambda I) \\\\ &=\\left|\\begin{array}{cc}{a_{11}-\\lambda} & {a_{12}} \\\\ {a_{21}} & {a_{22}-\\lambda}\\end{array}\\right| \\\\ &=\\left(a_{11}-\\lambda\\right)\\left(a_{22}-\\lambda\\right)-a_{12} a_{21} \\\\ &=\\lambda^{2}-\\underbrace{\\left(a_{11}+a_{22}\\right)}_{\\operatorname{Tr}(\\mathbf{A})} \\lambda+\\underbrace{\\left(a_{11} a_{22}-a_{12} a_{21}\\right)}_{\\operatorname{det}(\\mathbf{A})} \\end{aligned}$$\n",
    "\n",
    "The roots of the *characteristic polynomial* are the *eigenvalues* of the matrix $\\mathbf{A}$. Observe the coefficient of the linear term in $p_{\\mathbf{A}}(\\lambda)$ is equal to $- \\operatorname{Tr}(\\mathbf{A})$ and the constant term equals $\\operatorname{det}(\\mathbf{A})$. The name characteristic polynomial is indeed appropriate since $p_{\\mathbf{A}}(\\lambda)$ encodes the information about three important properties of the matrix $\\mathbf{A}$: its eigenvalues $(\\lambda_1,\\lambda_2)$, its trace $\\operatorname{Tr}(\\mathbf{A})$, and its determinant $\\operatorname{det}(\\mathbf{A})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 8., 0.]), array([[ 1.        ,  0.57735027, -0.81934191],\n",
       "        [ 0.        ,  0.57735027, -0.29496309],\n",
       "        [ 0.        ,  0.57735027,  0.49160514]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and eigenvectors\n",
    "\n",
    "The set of *eigenvectors* of a matrix is a special set of input vectors for which the action of the matrix is described as a simple *scaling*.\n",
    "\n",
    "Decomposing a matrix in terms of its *eigenvalues* and its *eigenvectors* gives valuable insights into the properties of the matrix. Certain matrix calculations, like computing the power of the matrix, become much easier when we use the *eigendecomposition* of the matrix.\n",
    "\n",
    "The best way to see what a matrix does is to look inside it and see what it’s made of. To understand the matrix A, you must find its *eigenvectors* and its *eigenvalues* (*eigen* is the German word for “self”). The *eigenvectors* of a matrix are a “natural basis” for describing the action of the the matrix. The *eigendecomposition* is a change-of-basis operation that expresses the matrix $\\mathbf{A}$ with respect to its *eigenbasis* (own-basis). The *eigendecomposition* of the matrix $\\mathbf{A}$ is a product of three matrices:\n",
    "\n",
    "$$\\mathbf{A}=\\left[\\begin{array}{rr}{9} & {-2} \\\\ {-2} & {6}\\end{array}\\right]=\\underbrace{\\left[\\begin{array}{cc}{1} & {2} \\\\ {2} & {-1}\\end{array}\\right]}_{\\mathbf{Q}} \\underbrace{\\left[\\begin{array}{cc}{5} & {0} \\\\ {0} & {10}\\end{array}\\right]}_{\\mathbf{\\Lambda}} \\underbrace{\\left[\\begin{array}{cc}{\\frac{1}{5}} & {\\frac{2}{5}} \\\\ {\\frac{2}{5}} & {-\\frac{1}{5}}\\end{array}\\right]}_{\\mathbf{Q^{-1}}}= \\mathbf{Q \\Lambda Q^{-1}}$$\n",
    "\n",
    "You can multiply the three matrices $\\mathbf{Q \\Lambda Q^{-1}}$ to obtain $\\mathbf{A}$. Note that the “middle matrix” $\\mathbf{\\Lambda}$ has entries only on the diagonal. The diagonal matrix $\\mathbf{\\Lambda}$ is sandwiched between the matrix $\\mathbf{Q}$ on the left and $\\mathbf{Q}^{-1}$ (the inverse of $\\mathbf{Q}$) on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing a matrix in terms of its eigenvalues and its eigenvectors is a powerful technique to “see inside” a matrix and understand what the matrix does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "* $\\mathbf{A}$: an $n \\times n$ square matrix. The entries of $\\mathbf{A}$ are denoted as $a_{ij}$\n",
    "\n",
    "* $\\operatorname{eig}(\\mathbf{A}) \\equiv (\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{n})$: the list of *eigenvalues* of $\\mathbf{A}$. *Eigenvalues* are usually denoted by the Greek letter lambda.\n",
    "\n",
    "* $p(\\lambda)=\\operatorname{det}(\\mathbf{A}-\\lambda \\mathbb{1})$: the *characteristic polynomial* of $\\mathbf{A}$. The *eigenvalues* of $\\mathbf{A}$ are the roots of the *characteristic polynomial*.\n",
    "\n",
    "* $\\left\\{\\vec{e}_{\\lambda_{1}}, \\vec{e}_{\\lambda_{2}}, \\dots, \\vec{e}_{\\lambda_{n}}\\right\\}$: the set of *eigenvectors* of $\\mathbf{A}$. Each *eigenvector* is associated with a corresponding *eigenvalue*.\n",
    "\n",
    "* $\\mathbf{\\Lambda} \\equiv \\operatorname{diag}\\left(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{n}\\right)$: the diagonalized version of $\\mathbf{A}$. The matrix $\\mathbf{\\Lambda}$ contains the eigenvalues of $\\mathbf{A}$ on the diagonal:\n",
    "$$ \\mathbf{\\Lambda} = \\left[\\begin{array}{ccc}{\\lambda_{1}} & {\\cdots} & {0} \\\\ {\\vdots} & {\\ddots} & {0} \\\\ {0} & {0} & {\\lambda_{n}}\\end{array}\\right]$$\n",
    "The matrix $\\mathbf{\\Lambda}$ is the matrix $\\mathbf{A}$ expressed in its *eigenbasis*.\n",
    "\n",
    "* $\\mathbf{Q}$: a matrix whose columns are eigenvectors of $\\mathbf{A}$:\n",
    "$$Q \\equiv\\left[\\begin{array}{ccc}{|} & {} & {|} \\\\ {\\vec{e}_{\\lambda_{1}}} & {\\cdots} & {\\vec{e}_{\\lambda_{n}}} \\\\ {|} & {} & {|}\\end{array}\\right]=_{B_{s}}[\\mathbb{1}]_{B_{\\lambda}}$$\n",
    "The matrix $\\mathbf{Q}$ corresponds to the *change-of-basis* matrix from the eigenbasis $B_{\\lambda}=\\left\\{\\vec{e}_{\\lambda_{1}}, \\vec{e}_{\\lambda_{2}}, \\vec{e}_{\\lambda_{3}}, \\dots\\right\\}$ to the standard basis $B_{s}=\\{\\hat{\\imath}, \\hat{\\jmath}, \\hat{k}, \\ldots\\}$.\n",
    "\n",
    "* $\\mathbf{A} = \\mathbf{Q \\Lambda Q^{-1}}$: the *eigendecomposition* of the matrix $\\mathbf{A}$\n",
    "\n",
    "* $\\mathbf{\\Lambda} = \\mathbf{Q^{-1} A Q}$: the *diagonalization* of the matrix $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues\n",
    "The fundamental eigenvalue equation is\n",
    "\n",
    "$$\\mathbf{A} \\vec{e}_{\\lambda}=\\lambda \\vec{e}_{\\lambda},$$\n",
    "\n",
    "where $\\lambda$ is an eigenvalue and $\\vec{e}_{\\lambda}$ is an eigenvector of the matrix $\\mathbf{A}$. Multiply $\\mathbf{A}$ by one of its eigenvectors $\\vec{e}_{\\lambda}$, and the result is the same vector scaled by the constant $\\lambda$.\n",
    "\n",
    "To find the eigenvalues of a matrix, start from the eigenvalue equation $\\mathbf{A} \\vec{e}_{\\lambda}=\\lambda \\vec{e}_{\\lambda}$, insert the identity $\\mathbb{1}$, and rewrite the equation as a null-space problem:\n",
    "\n",
    "$$\\mathbf{A} \\vec{e}_{\\lambda}=\\lambda \\mathbb{1} \\vec{e}_{\\lambda} \\quad \\Rightarrow \\quad(A-\\lambda \\mathbb{1}) \\vec{e}_{\\lambda}=\\overrightarrow{0}$$\n",
    "\n",
    "This equation has a solution whenever $|\\mathbf{A}-\\lambda \\mathbb{1}|=0$. The eigenvalues of $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, denoted $(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$, are the roots of the *characteristic polynomial*:\n",
    "\n",
    "$$p(\\lambda) \\equiv \\operatorname{det}(\\mathbf{A}-\\lambda \\mathbb{1})=0$$\n",
    "\n",
    "Calculate this determinant and we obtain an expression involving the coefficients $a_{ij}$ and the variable $\\lambda$. If $\\mathbf{A}$ is an $n \\times n$ matrix, the characteristic polynomial is a polynomial of degree $n$ in $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 8., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "The *eigenvectors* associated with eigenvalue $\\lambda_i$ of matrix $\\mathbf{A}$ are the vectors in the null space of the matrix $(\\mathbf{A} − \\lambda_i \\mathbb{1})$.\n",
    "\n",
    "To find the eigenvectors associated with the eigenvalue $\\lambda_i$, you need to solve for the components $e_{\\lambda,x}$ and $e_{\\lambda,y}$ of the vector $\\vec{e_{\\lambda}} = (e_{\\lambda,x}, e_{\\lambda,u})$ that satisfies the equation\n",
    "\n",
    "$$\\mathbf{A} \\vec{e}_{\\lambda}=\\lambda \\vec{e}_{\\lambda},$$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$(\\mathbf{A}-\\lambda \\mathbb{1}) \\vec{e}_{\\lambda}=0 \\quad \\Leftrightarrow \\quad\\left[\\begin{array}{cc}{a_{11}-\\lambda} & {a_{12}} \\\\ {a_{21}} & {a_{22}-\\lambda}\\end{array}\\right]\\left[\\begin{array}{c}{e_{\\lambda, x}} \\\\ {e_{\\lambda, y}}\\end{array}\\right]=\\left[\\begin{array}{l}{0} \\\\ {0}\\end{array}\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 8., 0.]), array([[ 1.        ,  0.57735027, -0.81934191],\n",
       "        [ 0.        ,  0.57735027, -0.29496309],\n",
       "        [ 0.        ,  0.57735027,  0.49160514]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,5],\n",
    "              [0,5,3],\n",
    "              [0,5,3]])\n",
    "\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "If an $n \\times n$ matrix $\\mathbf{A}$ is *diagonalizable* this means we can find $n$ eigenvectors for that matrix. The eigenvectors that come from different eigenspaces are guaranteed to be linearly independent. We can also pick a set of linearly independent vectors *within* each of the degenerate eigenspaces. Combining the eigenvectors from all the eigenspaces gives us a set of $n$ linearly independent eigenvectors, which form a *basis* for $\\mathbb{R}^n$. This is the *eigenbasis*.\n",
    "\n",
    "Let’s place the $n$ eigenvectors side by side as the columns of a\n",
    "matrix:\n",
    "\n",
    "$$\\mathbf{Q} \\equiv\\left[\\begin{array}{ccc}{|} & {} & {|} \\\\ {\\vec{e}_{\\lambda_{1}}} & {\\cdots} & {\\vec{e}_{\\lambda_{n}}} \\\\ {|} & {} & {|}\\end{array}\\right]$$\n",
    "\n",
    "We can decompose $\\mathbf{A}$ in terms of its eigenvalues and its eigenvectors:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{Q \\Lambda Q^{-1}}=\\left[\\begin{array}{ccc}{|} & {} & {|} \\\\ {\\vec{e}_{\\lambda_{1}}} & {\\cdots} & {\\vec{e}_{\\lambda_{n}}} \\\\ {|} & {} & {|}\\end{array}\\right]\\left[\\begin{array}{ccc}{\\lambda_{1}} & {\\cdots} & {0} \\\\ {\\vdots} & {\\ddots} & {0} \\\\ {0} & {0} & {\\lambda_{n}}\\end{array}\\right]\\left[\\begin{array}{c} {}{}{}\\\\{}{\\mathcal{Q}^{-1}}{}\\\\{}{}{}\\end{array}\\right]$$\n",
    "\n",
    "The matrix $\\mathbf{\\Lambda}$ is a diagonal matrix of eigenvalues, and the matrix $\\mathbf{Q}$ is the change-of-basis matrix that contains the corresponding eigenvectors as columns.\n",
    "\n",
    "Note that only the direction of each eigenvector is important and not the length. Indeed, if $\\vec{e}_\\lambda$ is an eigenvector (with eigenvalue $\\lambda$), so is any $\\alpha \\vec{e}_\\lambda$ for all $\\alpha \\in \\mathbb{R}$. Thus we’re free to use any multiple of the vectors $\\vec{e}_{\\lambda_{i}}$ as the columns of the matrix $\\mathbf{Q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "* **Statistics**: *Principal Component Analysis* technique aims to uncover the dominant cause of the variation in datasets by eigendecomposing the *covariance matrix*--a matrix computed from the dataset.\n",
    "\n",
    "* **Industry**: Google’s original `PageRank` algorithm for ranking webpages by “importance” can be explained as the search for an eigenvector of a matrix. The matrix contains information about all the hyperlinks that exist between webpages.\n",
    "\n",
    "* **Physics**: In quantum mechanics, the energy of a system is described by the Hamiltonian operator. The eigenvalues of the Hamiltonian are the possible energy levels the system can have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Types Matrices\n",
    "\n",
    "* **Diagonal Matrices**: Diagonal matrices contain entries on the diagonal and zeros everywhere else:\n",
    "$$\\mathbf{A}=\\left[\\begin{array}{ccc}{a_{11}} & {0} & {0} \\\\ {0} & {a_{22}} & {0} \\\\ {0} & {0} & {a_{33}}\\end{array}\\right]$$\n",
    "\n",
    "* **Symmetric Matrices**:  A matrix $\\mathbf{A}$ is symmetric if and only if\n",
    "$$\\mathbf{A}^{\\top}=\\mathbf{A}, \\quad \\text { or equivalently if } \\quad a_{i j}=a_{j i}, \\text { for all } i, j$$\n",
    "\n",
    "* **Upper Triangular Matrices**: Upper triangular matrices have zero entries below the main diagonal:\n",
    "$$\\left[\\begin{array}{ccc}{a_{11}} & {a_{12}} & {a_{13}} \\\\ {0} & {a_{22}} & {a_{23}} \\\\ {0} & {0} & {a_{33}}\\end{array}\\right], \\quad a_{i j}=0, \\quad \\text { if } i>j$$\n",
    "\n",
    "* **Identity Matrix**: The identity matrix is denoted $\\mathbb{1}$ or $\\mathbb{1}_n \\in \\mathbb{R}^{n \\times n}$ and plays the role of multiplication by the number 1 for matrices: $\\mathbb{1} \\mathbf{A} = \\mathbf{A} \\mathbb{1} = \\mathbf{A}$. The identity matrix is diagonal with ones on the diagonal:\n",
    "$$\\mathbb{1}_{3}=\\left[\\begin{array}{lll}{1} & {0} & {0} \\\\ {0} & {1} & {0} \\\\ {0} & {0} & {1}\\end{array}\\right]$$\n",
    "\n",
    "* **Orthogonal Matrices**: A matrix $\\mathbf{O} \\in \\mathbb{R}^{n \\times n}$ is *orthogonal* if it satisfies $\\mathbf{OO}^T = \\mathbb{1} = \\mathbf{O}^T\\mathbf{O}$. In other words, the inverse of an orthogonal matrix $\\mathbf{O}$ is obtained by taking its transpose: $\\mathbf{O}^{-1} = \\mathbf{O}^T$. You can visualize orthogonal matrices by thinking of their columns as a set of vectors that form an orthonormal basis for $\\mathbb{R}^n$:\n",
    "$$\\mathbf{O}=\\left[\\begin{array}{lll}{|} & {} & {|} \\\\ {\\hat{e}_{1}} & {\\cdots} & {\\hat{e}_{n}} \\\\ {|} & {} & {|}\\end{array}\\right] \\quad \\text { such that } \\quad \\hat{e}_{i} \\cdot \\hat{e}_{j}=\\left\\{\\begin{array}{ll}{1} & {\\text { if } i=j} \\\\ {0} & {\\text { if } i \\neq j}\\end{array}\\right.$$\n",
    "You can verify the matrix $\\mathbf{O}$ is orthogonal by computing $\\mathbf{O}^T\\mathbf{O} = 1$.\n",
    "\n",
    "* **Rotation Matrices**: A rotation matrix takes the standard basis $\\{\\hat{\\imath}, \\hat{\\jmath}, \\hat{k}\\}$ to a rotated basis $\\left\\{\\hat{e}_{1}, \\hat{e}_{2}, \\hat{e}_{3}\\right\\}$. The determinant of a rotation matrix is equal to one. The eigenvalues of rotation matrices are complex numbers with unit magnitude.\n",
    "\n",
    "* **Reflections**: If the determinant of an orthogonal matrix $\\mathbf{O}$ is equal to negative one, we say it is *mirrored orthogonal*. A reflection matrix always has at least one eigenvalue equal to negative one, which corresponds to the direction perpendicular to the axis of reflection.\n",
    "\n",
    "* **Permutation Matrices**: Permutation matrices are another important class of orthogonal matrices. The action of a permutation matrix is simply to change the *order* of the coefficients of a vector. An $n \\times n$ permutation matrix contains $n$ ones in $n$ different columns and zeros everywhere else. The sign of a permutation corresponds to the determinant $\\operatorname{det}\\left(M_{\\pi}\\right)$. We say that permutation $\\pi$ is *even* if $\\operatorname{det}\\left(M_{\\pi}\\right) = +1$ and *odd* if $\\operatorname{det}\\left(M_{\\pi}\\right) = −1$.\n",
    "\n",
    "* **Positive Matrices**: The defining property of a projection matrix is that it can be applied multiple times without changing the result:\n",
    "$$\\Pi=\\Pi^{2}=\\Pi^{3}=\\Pi^{4}=\\Pi^{5}=\\cdots$$\n",
    "A projection has two eigenvalues: one and zero. The space $S$ that is left invariant by the projection $\\Pi_S$ corresponds to the eigenvalue $\\lambda = 1$. The orthogonal complement $S^\\perp$ corresponds to the eigenvalue $\\lambda = 0$ and consists of vectors that get annihilated by $\\Pi_S$. The space $S^\\perp$ is the null space of $\\Pi_S$.\n",
    "\n",
    "* **Normal Matrices**: The matrix $\\mathbf{A} = \\mathbb{R}^{n \\times n}$ is normal if it obeys $\\mathbf{A}^T\\mathbf{A} = \\mathbf{A}\\mathbf{A}^T$. If $\\mathbf{A}$is normal, it has the following properties:\n",
    "    * $\\vec{v}$ is an eigenvector of $\\mathbf{A}$ if and only if $\\vec{v}$ is an eigenvector of $\\mathbf{A}^T$.\n",
    "    * For all vectors $\\vec{v}$ and $\\vec{w}$ and a normal transformation $\\mathbf{A}$, we have:\n",
    "    $$(\\mathbf{A} \\vec{v}) \\cdot(\\mathbf{A} \\vec{w})=(\\mathbf{A} \\vec{v})^{\\top}(\\mathbf{A} \\vec{w})=\\vec{v}^{\\top} \\mathbf{A}^{\\top} \\mathbf{A} \\vec{w}=\\vec{v}^{\\top} \\mathbf{A} \\mathbf{A}^{\\top} \\vec{w}=\\left(\\mathbf{A}^{\\top} \\vec{v}\\right) \\cdot\\left(\\mathbf{A}^{\\top} \\vec{w}\\right)$$\n",
    "    * The matrix $\\mathbf{A}$ has a full set of linearly independent eigenvectors. Eigenvectors corresponding to distinct eigenvalues are orthogonal and eigenvectors from the same eigenspace can be chosen to be mutually orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Product\n",
    "\n",
    "Take pairs of vectors as inputs and produces number as outputs:\n",
    "\n",
    "$$\\langle\\cdot, \\cdot\\rangle : V \\times V \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "We can use any inner product operation, as long as it satisfies the following criteria for all $u, v, v_1, v_2 \\in V$ and $\\alpha, \\beta \\in \\mathbb{R}$. The inner product operation must be:\n",
    "\n",
    "* **Symmetric**: $\\langle u, v \\rangle = \\langle v, u \\rangle$\n",
    "* **Linear**: $\\langle u, \\alpha v_1 + \\beta v_2 \\rangle = \\alpha \\langle u, v_1 \\rangle + \\beta \\langle u, v_2 \\rangle$\n",
    "* **Positive Semidefinite**: $\\langle u,u \\rangle \\geq 0$ for all $u \\in V$ with $\\langle u,u \\rangle = 0$ if and only if $u=0$\n",
    "\n",
    "### Dot Product\n",
    "It is the Inner Product for two vectors in $ \\mathbb{R}^n$:\n",
    "$$\\langle\\vec{u}, \\vec{v}\\rangle \\equiv \\vec{u} \\cdot \\vec{v}=\\sum_{i=1}^{n} u_{i} v_{i}=\\vec{u}^{\\top} \\vec{v}$$\n",
    "\n",
    "* **Orthogonality**: $u$ and $v$ are orthogonal if $\\langle u, v \\rangle = 0$\n",
    "* ***Norm* or Length of Vector**: $||u|| \\equiv \\sqrt{\\langle u, u \\rangle}$\n",
    "* **Distance between two vectors**: $d(u,v) \\equiv ||u-v|| = \\sqrt{\\langle (u-v), (u-v) \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([[1],\n",
    "              [1]])\n",
    "u = np.array([[1],\n",
    "              [1]])\n",
    "\n",
    "np.dot(v.T,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 1.4142135623730951$"
      ],
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 1.4142135623730951$"
      ],
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SciPy Spatial Distance\n",
    "spatial.distance.cdist(v,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Product\n",
    "\n",
    "The cross product of two vectors $u$ and $v$ is defined only in three-dimensional space and is denoted by $u \\times v$:\n",
    "\n",
    "$$u \\times v = ||u|| \\space ||v|| \\sin (\\theta) n,$$\n",
    "\n",
    "where $\\theta$ is the angle between $u$ and $v$, $||u||$ and $||v||$ are the magnitudes of vectors $u$ and $v$, and $n$ is a *unit vector* perpendicular to the plane containing $u$ and $v$.\n",
    "\n",
    "### Orthogonal\n",
    "The cross product $u \\times v$ is defined as a vector $c$ that is *orthogonal* (perpendicular) to both $u$ and $v$ with magnitude given by the area of the *parallelogram* that the vectors span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3,  6, -3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1,2,3])\n",
    "u = np.array([4,5,6])\n",
    "\n",
    "np.cross(v,u)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
